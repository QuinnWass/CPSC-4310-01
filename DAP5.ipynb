{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAP5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuinnWass/CPSC-4310-01/blob/master/DAP5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0-DxhRjgWvr",
        "colab_type": "text"
      },
      "source": [
        "# DAP5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-bHdq2bgZpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, tqdm_gui, tqdm_notebook\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "%matplotlib inline \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize = 14)\n",
        "mpl.rc('xtick', labelsize = 12)\n",
        "mpl.rc('ytick', labelsize = 12)\n",
        "\n",
        "\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWqGmfTkga_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parts_of_speech = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ',\n",
        "                    'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP',\n",
        "                    'NNPS', 'PDT', 'PRP$', 'RB', 'RBR', 'PRP', 'RBS',\n",
        "                    'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP',\n",
        "                    'VBZ', 'WDT', 'WP', 'WP$', 'WRB' ]\n",
        "\n",
        "\n",
        "\n",
        "#Load dataset into df\n",
        "dataset = 'https://raw.githubusercontent.com/QuinnWass/CPSC-4310-01/master/FakeNewsClassifier/processeddata/cb_full_parsed.csv'\n",
        "df = pd.read_csv(dataset)\n",
        "\n",
        "\n",
        "#this is to get rid  https://seattleu.zoom.us/j/5664223549 of the XX.1 thing we have going on for some reason when\n",
        "#the dataframe is loaded.\n",
        "df = df.drop(columns = parts_of_speech)\n",
        "parts_of_speech_dict = {'CC.1':'CC', 'CD.1':'CD', 'DT.1':'DT', 'EX.1':'EX', 'FW.1':'FW', 'IN.1':'IN', 'JJ.1':'JJ', 'JJR.1':'JJR', 'JJS.1':'JJS', 'LS.1':'LS', 'MD.1':'MD',\n",
        "                   'NN.1':'NN', 'NNS.1':'NNS', 'NNP.1':'NNP', \\\n",
        "                    'NNPS.1':'NNPS', 'PDT.1':'PDT', 'PRP$.1':'PRP$', 'RB.1':'RB', 'RBR.1':'RBR', 'PRP.1':'PRP', 'RBS.1':'RBS', \\\n",
        "                    'RP.1':'RP', 'TO.1':'TO', 'UH.1':'UH', 'VB.1':'VB', 'VBD.1':'VBD', 'VBG.1':'VBG', 'VBN.1':'VBN', 'VBP.1':'VBP', \\\n",
        "                    'VBZ.1':'VBZ', 'WDT.1':'WDT', 'WP.1':'WP', 'WP$.1':'WP$', 'WRB.1':'WRB' }\n",
        "df = df.rename(columns=parts_of_speech_dict)\n",
        "\n",
        "\n",
        "\n",
        "df.classification = pd.Categorical(df.classification)\n",
        "df['code'] = df.classification.cat.codes\n",
        "\n",
        "#appending additional features to the parts of speech array\n",
        "#as our exploratory analysis has shown they may be a factor\n",
        "features = parts_of_speech + ['word_count', 'title_sentiment', 'sentiment']\n",
        "X_df = df[features]\n",
        "Y_df = df['code']\n",
        "\n",
        "X = X_df.values\n",
        "y = Y_df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to6Oqx-Pjq0k",
        "colab_type": "code",
        "outputId": "bf21a52c-1570-4518-a234-a9d8b00da557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>word_count</th>\n",
              "      <th>title_word_count</th>\n",
              "      <th>title_sentiment</th>\n",
              "      <th>CC</th>\n",
              "      <th>CD</th>\n",
              "      <th>DT</th>\n",
              "      <th>EX</th>\n",
              "      <th>FW</th>\n",
              "      <th>IN</th>\n",
              "      <th>JJ</th>\n",
              "      <th>JJR</th>\n",
              "      <th>JJS</th>\n",
              "      <th>LS</th>\n",
              "      <th>MD</th>\n",
              "      <th>NN</th>\n",
              "      <th>NNS</th>\n",
              "      <th>NNP</th>\n",
              "      <th>NNPS</th>\n",
              "      <th>PDT</th>\n",
              "      <th>PRP$</th>\n",
              "      <th>RB</th>\n",
              "      <th>RBR</th>\n",
              "      <th>PRP</th>\n",
              "      <th>RBS</th>\n",
              "      <th>RP</th>\n",
              "      <th>TO</th>\n",
              "      <th>UH</th>\n",
              "      <th>VB</th>\n",
              "      <th>VBD</th>\n",
              "      <th>VBG</th>\n",
              "      <th>VBN</th>\n",
              "      <th>VBP</th>\n",
              "      <th>VBZ</th>\n",
              "      <th>WDT</th>\n",
              "      <th>WP</th>\n",
              "      <th>WP$</th>\n",
              "      <th>WRB</th>\n",
              "      <th>code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>44267.00000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.0</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "      <td>44267.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>22133.00000</td>\n",
              "      <td>0.016331</td>\n",
              "      <td>252.014977</td>\n",
              "      <td>11.576841</td>\n",
              "      <td>-0.124848</td>\n",
              "      <td>0.889037</td>\n",
              "      <td>2.749723</td>\n",
              "      <td>4.478325</td>\n",
              "      <td>0.096731</td>\n",
              "      <td>0.327806</td>\n",
              "      <td>6.850792</td>\n",
              "      <td>47.862810</td>\n",
              "      <td>0.676554</td>\n",
              "      <td>0.765807</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.605349</td>\n",
              "      <td>110.977455</td>\n",
              "      <td>7.749994</td>\n",
              "      <td>0.499605</td>\n",
              "      <td>0.074977</td>\n",
              "      <td>0.061716</td>\n",
              "      <td>0.278560</td>\n",
              "      <td>11.879233</td>\n",
              "      <td>0.467549</td>\n",
              "      <td>2.704453</td>\n",
              "      <td>0.044661</td>\n",
              "      <td>0.223643</td>\n",
              "      <td>0.109924</td>\n",
              "      <td>0.018818</td>\n",
              "      <td>5.376578</td>\n",
              "      <td>16.949714</td>\n",
              "      <td>10.382791</td>\n",
              "      <td>6.173944</td>\n",
              "      <td>7.937606</td>\n",
              "      <td>2.191113</td>\n",
              "      <td>0.160503</td>\n",
              "      <td>0.192581</td>\n",
              "      <td>0.081212</td>\n",
              "      <td>0.269998</td>\n",
              "      <td>0.483792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>12778.92652</td>\n",
              "      <td>0.841605</td>\n",
              "      <td>216.469833</td>\n",
              "      <td>4.551793</td>\n",
              "      <td>0.427399</td>\n",
              "      <td>1.569163</td>\n",
              "      <td>3.401539</td>\n",
              "      <td>4.763000</td>\n",
              "      <td>0.341811</td>\n",
              "      <td>0.930322</td>\n",
              "      <td>7.026680</td>\n",
              "      <td>43.488887</td>\n",
              "      <td>1.159265</td>\n",
              "      <td>1.223750</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.265678</td>\n",
              "      <td>97.027810</td>\n",
              "      <td>7.335827</td>\n",
              "      <td>1.095131</td>\n",
              "      <td>0.337298</td>\n",
              "      <td>0.252462</td>\n",
              "      <td>0.681485</td>\n",
              "      <td>12.455623</td>\n",
              "      <td>0.822224</td>\n",
              "      <td>3.610236</td>\n",
              "      <td>0.222056</td>\n",
              "      <td>0.548096</td>\n",
              "      <td>0.439312</td>\n",
              "      <td>0.155570</td>\n",
              "      <td>5.425026</td>\n",
              "      <td>14.853689</td>\n",
              "      <td>9.974610</td>\n",
              "      <td>6.380545</td>\n",
              "      <td>7.524627</td>\n",
              "      <td>2.631081</td>\n",
              "      <td>0.458720</td>\n",
              "      <td>0.572871</td>\n",
              "      <td>0.313085</td>\n",
              "      <td>0.667432</td>\n",
              "      <td>0.499743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.975000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>11066.50000</td>\n",
              "      <td>-0.936000</td>\n",
              "      <td>132.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>-0.476700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>22133.00000</td>\n",
              "      <td>0.077200</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>33199.50000</td>\n",
              "      <td>0.926000</td>\n",
              "      <td>314.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.102700</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>139.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>44266.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5193.000000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>0.970500</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>165.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>175.000000</td>\n",
              "      <td>1145.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>3028.000000</td>\n",
              "      <td>211.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>280.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>114.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>145.000000</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0     sentiment  ...           WRB          code\n",
              "count  44267.00000  44267.000000  ...  44267.000000  44267.000000\n",
              "mean   22133.00000      0.016331  ...      0.269998      0.483792\n",
              "std    12778.92652      0.841605  ...      0.667432      0.499743\n",
              "min        0.00000     -1.000000  ...      0.000000      0.000000\n",
              "25%    11066.50000     -0.936000  ...      0.000000      0.000000\n",
              "50%    22133.00000      0.077200  ...      0.000000      0.000000\n",
              "75%    33199.50000      0.926000  ...      0.000000      1.000000\n",
              "max    44266.00000      1.000000  ...     14.000000      1.000000\n",
              "\n",
              "[8 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jrc2_Qs_go2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    y,\n",
        "                                                    test_size = 0.33,\n",
        "                                                    random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yStyQbehM_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import `StandardScaler` from `sklearn.preprocessing`\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the scaler \n",
        "scaler = StandardScaler().fit(X_train)\n",
        "\n",
        "# Scale the train set\n",
        "X_train = scaler.transform(X_train)\n",
        "\n",
        "# Scale the test set\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu_w7_VVn4nX",
        "colab_type": "text"
      },
      "source": [
        "## Model A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2dwkwfrhUIu",
        "colab_type": "code",
        "outputId": "05640bf4-c8ae-48c5-e806-ce80b0a33011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "# construct a model\n",
        "\n",
        "modelA = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "modelA.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modelA.fit(X_train, y_train, epochs=10, batch_size=50, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelA.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "594/594 [==============================] - 1s 2ms/step - loss: 0.4399 - accuracy: 0.7976\n",
            "Epoch 2/10\n",
            "594/594 [==============================] - 1s 2ms/step - loss: 0.3421 - accuracy: 0.8551\n",
            "Epoch 3/10\n",
            "594/594 [==============================] - 1s 2ms/step - loss: 0.3289 - accuracy: 0.8585\n",
            "Epoch 4/10\n",
            "594/594 [==============================] - 1s 2ms/step - loss: 0.3216 - accuracy: 0.8643\n",
            "Epoch 5/10\n",
            "594/594 [==============================] - 1s 2ms/step - loss: 0.3153 - accuracy: 0.8646\n",
            "Epoch 6/10\n",
            "594/594 [==============================] - 1s 2ms/step - loss: 0.3117 - accuracy: 0.8657\n",
            "Epoch 7/10\n",
            "594/594 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.8684\n",
            "Epoch 8/10\n",
            "594/594 [==============================] - 1s 2ms/step - loss: 0.3041 - accuracy: 0.8714\n",
            "Epoch 9/10\n",
            "594/594 [==============================] - 1s 2ms/step - loss: 0.3010 - accuracy: 0.8738\n",
            "Epoch 10/10\n",
            "594/594 [==============================] - 1s 2ms/step - loss: 0.2975 - accuracy: 0.8735\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 2ms/step - loss: 0.3216 - accuracy: 0.8645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.32164573669433594, 0.8645355701446533]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H48OHNcXoyO_",
        "colab_type": "text"
      },
      "source": [
        "## Model B Extra layer, more nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvocZQdSn6la",
        "colab_type": "code",
        "outputId": "4ce62183-1525-4c4a-d834-d0404340d4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "# construct a model\n",
        "\n",
        "modelB = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(2 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(3 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "modelB.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modelB.fit(X_train, y_train, epochs=10, batch_size=100, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelB.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4332 - accuracy: 0.8049\n",
            "Epoch 2/10\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.3462 - accuracy: 0.8535\n",
            "Epoch 3/10\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8605\n",
            "Epoch 4/10\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.3211 - accuracy: 0.8660\n",
            "Epoch 5/10\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.3110 - accuracy: 0.8708\n",
            "Epoch 6/10\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.3063 - accuracy: 0.8725\n",
            "Epoch 7/10\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.3002 - accuracy: 0.8750\n",
            "Epoch 8/10\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.2956 - accuracy: 0.8765\n",
            "Epoch 9/10\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.2894 - accuracy: 0.8801\n",
            "Epoch 10/10\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.2836 - accuracy: 0.8824\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 2ms/step - loss: 0.3191 - accuracy: 0.8633\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31907206773757935, 0.8633034229278564]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-PjKezKo4Bd",
        "colab_type": "text"
      },
      "source": [
        "## Model C Wide\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEuzm4k1o70O",
        "colab_type": "code",
        "outputId": "623f4072-b238-498e-bf1d-5c26d3cda1ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# construct a model\n",
        "modelB = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "modelB.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modelB.fit(X_train, y_train, epochs=100, batch_size=100, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelB.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.4304 - accuracy: 0.8037\n",
            "Epoch 2/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.3482 - accuracy: 0.8540\n",
            "Epoch 3/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.3307 - accuracy: 0.8612\n",
            "Epoch 4/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.3114 - accuracy: 0.8697\n",
            "Epoch 5/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.2984 - accuracy: 0.8747\n",
            "Epoch 6/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.2868 - accuracy: 0.8795\n",
            "Epoch 7/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.2746 - accuracy: 0.8854\n",
            "Epoch 8/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.2610 - accuracy: 0.8908\n",
            "Epoch 9/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.2439 - accuracy: 0.8972\n",
            "Epoch 10/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.2270 - accuracy: 0.9043\n",
            "Epoch 11/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.2110 - accuracy: 0.9117\n",
            "Epoch 12/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.1920 - accuracy: 0.9218\n",
            "Epoch 13/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.1665 - accuracy: 0.9315\n",
            "Epoch 14/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.1463 - accuracy: 0.9418\n",
            "Epoch 15/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.1270 - accuracy: 0.9491\n",
            "Epoch 16/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.1111 - accuracy: 0.9584\n",
            "Epoch 17/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0985 - accuracy: 0.9619\n",
            "Epoch 18/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0878 - accuracy: 0.9660\n",
            "Epoch 19/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0787 - accuracy: 0.9710\n",
            "Epoch 20/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0667 - accuracy: 0.9756\n",
            "Epoch 21/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0654 - accuracy: 0.9767\n",
            "Epoch 22/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0544 - accuracy: 0.9791\n",
            "Epoch 23/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0581 - accuracy: 0.9788\n",
            "Epoch 24/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0506 - accuracy: 0.9817\n",
            "Epoch 25/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0471 - accuracy: 0.9827\n",
            "Epoch 26/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0412 - accuracy: 0.9858\n",
            "Epoch 27/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0430 - accuracy: 0.9842\n",
            "Epoch 28/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0453 - accuracy: 0.9844\n",
            "Epoch 29/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0388 - accuracy: 0.9858\n",
            "Epoch 30/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0376 - accuracy: 0.9863\n",
            "Epoch 31/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0383 - accuracy: 0.9864\n",
            "Epoch 32/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0385 - accuracy: 0.9872\n",
            "Epoch 33/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0366 - accuracy: 0.9873\n",
            "Epoch 34/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0331 - accuracy: 0.9889\n",
            "Epoch 35/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0304 - accuracy: 0.9896\n",
            "Epoch 36/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0293 - accuracy: 0.9903\n",
            "Epoch 37/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0273 - accuracy: 0.9905\n",
            "Epoch 38/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0275 - accuracy: 0.9899\n",
            "Epoch 39/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0266 - accuracy: 0.9902\n",
            "Epoch 40/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0308 - accuracy: 0.9895\n",
            "Epoch 41/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0290 - accuracy: 0.9901\n",
            "Epoch 42/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0260 - accuracy: 0.9918\n",
            "Epoch 43/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0218 - accuracy: 0.9926\n",
            "Epoch 44/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0291 - accuracy: 0.9902\n",
            "Epoch 45/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0236 - accuracy: 0.9917\n",
            "Epoch 46/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0210 - accuracy: 0.9929\n",
            "Epoch 47/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0195 - accuracy: 0.9936\n",
            "Epoch 48/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0276 - accuracy: 0.9909\n",
            "Epoch 49/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0291 - accuracy: 0.9899\n",
            "Epoch 50/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0223 - accuracy: 0.9922\n",
            "Epoch 51/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0204 - accuracy: 0.9925\n",
            "Epoch 52/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0180 - accuracy: 0.9942\n",
            "Epoch 53/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0259 - accuracy: 0.9907\n",
            "Epoch 54/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0220 - accuracy: 0.9927\n",
            "Epoch 55/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0199 - accuracy: 0.9938\n",
            "Epoch 56/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0188 - accuracy: 0.9937\n",
            "Epoch 57/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0235 - accuracy: 0.9920\n",
            "Epoch 58/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0261 - accuracy: 0.9912\n",
            "Epoch 59/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0189 - accuracy: 0.9940\n",
            "Epoch 60/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0180 - accuracy: 0.9945\n",
            "Epoch 61/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0205 - accuracy: 0.9932\n",
            "Epoch 62/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0174 - accuracy: 0.9939\n",
            "Epoch 63/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0224 - accuracy: 0.9927\n",
            "Epoch 64/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0262 - accuracy: 0.9910\n",
            "Epoch 65/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0173 - accuracy: 0.9941\n",
            "Epoch 66/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0135 - accuracy: 0.9957\n",
            "Epoch 67/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0158 - accuracy: 0.9950\n",
            "Epoch 68/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0157 - accuracy: 0.9950\n",
            "Epoch 69/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0178 - accuracy: 0.9948\n",
            "Epoch 70/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0150 - accuracy: 0.9949\n",
            "Epoch 71/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0165 - accuracy: 0.9951\n",
            "Epoch 72/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0166 - accuracy: 0.9950\n",
            "Epoch 73/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0133 - accuracy: 0.9952\n",
            "Epoch 74/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0170 - accuracy: 0.9949\n",
            "Epoch 75/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0177 - accuracy: 0.9948\n",
            "Epoch 76/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0194 - accuracy: 0.9942\n",
            "Epoch 77/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0127 - accuracy: 0.9962\n",
            "Epoch 78/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0157 - accuracy: 0.9949\n",
            "Epoch 79/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0099 - accuracy: 0.9970\n",
            "Epoch 80/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0153 - accuracy: 0.9960\n",
            "Epoch 81/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0209 - accuracy: 0.9934\n",
            "Epoch 82/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0188 - accuracy: 0.9944\n",
            "Epoch 83/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0187 - accuracy: 0.9937\n",
            "Epoch 84/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0144 - accuracy: 0.9955\n",
            "Epoch 85/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0145 - accuracy: 0.9950\n",
            "Epoch 86/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0141 - accuracy: 0.9954\n",
            "Epoch 87/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0111 - accuracy: 0.9961\n",
            "Epoch 88/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0175 - accuracy: 0.9948\n",
            "Epoch 89/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0098 - accuracy: 0.9969\n",
            "Epoch 90/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0098 - accuracy: 0.9967\n",
            "Epoch 91/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0174 - accuracy: 0.9948\n",
            "Epoch 92/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0263 - accuracy: 0.9946\n",
            "Epoch 93/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0146 - accuracy: 0.9956\n",
            "Epoch 94/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0104 - accuracy: 0.9973\n",
            "Epoch 95/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0112 - accuracy: 0.9968\n",
            "Epoch 96/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0140 - accuracy: 0.9958\n",
            "Epoch 97/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0114 - accuracy: 0.9968\n",
            "Epoch 98/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0110 - accuracy: 0.9967\n",
            "Epoch 99/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0176 - accuracy: 0.9948\n",
            "Epoch 100/100\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.0131 - accuracy: 0.9966\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 2ms/step - loss: 0.9664 - accuracy: 0.8869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9663607478141785, 0.8868505954742432]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ufZCufZQiYh",
        "colab_type": "text"
      },
      "source": [
        "As can be seen, this model grossly overfits and has a high loss during evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFE9h2vMQpNF",
        "colab_type": "text"
      },
      "source": [
        "## Model C Reduced Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4t3R6yTQsQ5",
        "colab_type": "code",
        "outputId": "9bc50c29-d617-4d1e-a1d1-a9ab186725b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# construct a model\n",
        "modelB = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "modelB.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modelB.fit(X_train, y_train, epochs=25, batch_size=100, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelB.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.4323 - accuracy: 0.8062\n",
            "Epoch 2/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.3467 - accuracy: 0.8547\n",
            "Epoch 3/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.3272 - accuracy: 0.8626\n",
            "Epoch 4/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.3145 - accuracy: 0.8678\n",
            "Epoch 5/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.3019 - accuracy: 0.8734\n",
            "Epoch 6/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.2943 - accuracy: 0.8757\n",
            "Epoch 7/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.2803 - accuracy: 0.8812\n",
            "Epoch 8/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.2630 - accuracy: 0.8882\n",
            "Epoch 9/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.2487 - accuracy: 0.8942\n",
            "Epoch 10/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.2327 - accuracy: 0.9010\n",
            "Epoch 11/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.2143 - accuracy: 0.9098\n",
            "Epoch 12/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.1917 - accuracy: 0.9206\n",
            "Epoch 13/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.1747 - accuracy: 0.9286\n",
            "Epoch 14/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.1504 - accuracy: 0.9397\n",
            "Epoch 15/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.1282 - accuracy: 0.9484\n",
            "Epoch 16/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.1149 - accuracy: 0.9562\n",
            "Epoch 17/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.1071 - accuracy: 0.9596\n",
            "Epoch 18/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.0851 - accuracy: 0.9672\n",
            "Epoch 19/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.0797 - accuracy: 0.9698\n",
            "Epoch 20/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.0637 - accuracy: 0.9759\n",
            "Epoch 21/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.0601 - accuracy: 0.9775\n",
            "Epoch 22/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.0549 - accuracy: 0.9793\n",
            "Epoch 23/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.0531 - accuracy: 0.9816\n",
            "Epoch 24/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.0471 - accuracy: 0.9818\n",
            "Epoch 25/25\n",
            "297/297 [==============================] - 2s 8ms/step - loss: 0.0444 - accuracy: 0.9838\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 3ms/step - loss: 0.7798 - accuracy: 0.8795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7797792553901672, 0.8794578909873962]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5_r_GP7RKsT",
        "colab_type": "text"
      },
      "source": [
        "As can be seen, the loss is still high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVXQc37eSLnr",
        "colab_type": "text"
      },
      "source": [
        "## Model D\n",
        "Reducing batch size of model B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjjOlrmySMDa",
        "colab_type": "code",
        "outputId": "526e7cab-4ee3-47ee-f6b4-8f36f9c5fded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "# construct a model\n",
        "\n",
        "modelD = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(2 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(3 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(3 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(3 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "modelD.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modelD.fit(X_train, y_train, epochs=10, batch_size=1, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelD.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.4241 - accuracy: 0.8254\n",
            "Epoch 2/10\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.2102 - accuracy: 0.9269\n",
            "Epoch 3/10\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.1588 - accuracy: 0.9552\n",
            "Epoch 4/10\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.1222 - accuracy: 0.9537\n",
            "Epoch 5/10\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.0772 - accuracy: 0.9746\n",
            "Epoch 6/10\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.1137 - accuracy: 0.9627\n",
            "Epoch 7/10\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.0794 - accuracy: 0.9716\n",
            "Epoch 8/10\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.0514 - accuracy: 0.9776\n",
            "Epoch 9/10\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.0487 - accuracy: 0.9896\n",
            "Epoch 10/10\n",
            "670/670 [==============================] - 2s 3ms/step - loss: 0.0490 - accuracy: 0.9851\n",
            "Evaluate\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.3083 - accuracy: 0.9364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3083204925060272, 0.9363636374473572]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avaG1paDTa_d",
        "colab_type": "text"
      },
      "source": [
        "# Model E"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONiEL5yoTaDX",
        "colab_type": "code",
        "outputId": "61a7bf4a-9292-4d74-9350-a090c84d8d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "# construct a model\n",
        "\n",
        "modelE = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='sigmoid', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(2 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(2 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(2 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(2 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(2 * len(X_df.columns), activation='sigmoid'),\n",
        "    keras.layers.Dense(3 * len(X_df.columns), activation='swish'),\n",
        "    keras.layers.Dense(2 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(3 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(3 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(3 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "modelE.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', \n",
        "                                                                      tf.keras.metrics.AUC()])\n",
        "\n",
        "modelE.fit(X_train, y_train, epochs=10, batch_size=15, validation_split=0.2, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelE.evaluate(X_test, y_test, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1370/1370 [==============================] - 6s 5ms/step - loss: 0.4310 - accuracy: 0.7983 - auc_22: 0.8807 - val_loss: 0.3576 - val_accuracy: 0.8497 - val_auc_22: 0.9258\n",
            "Epoch 2/10\n",
            "1370/1370 [==============================] - 6s 4ms/step - loss: 0.3693 - accuracy: 0.8430 - auc_22: 0.9143 - val_loss: 0.4215 - val_accuracy: 0.8218 - val_auc_22: 0.9294\n",
            "Epoch 3/10\n",
            "1370/1370 [==============================] - 7s 5ms/step - loss: 0.3580 - accuracy: 0.8469 - auc_22: 0.9194 - val_loss: 0.3830 - val_accuracy: 0.8403 - val_auc_22: 0.9297\n",
            "Epoch 4/10\n",
            "1370/1370 [==============================] - 6s 5ms/step - loss: 0.3510 - accuracy: 0.8501 - auc_22: 0.9227 - val_loss: 0.3347 - val_accuracy: 0.8545 - val_auc_22: 0.9320\n",
            "Epoch 5/10\n",
            "1370/1370 [==============================] - 6s 4ms/step - loss: 0.3509 - accuracy: 0.8497 - auc_22: 0.9226 - val_loss: 0.3574 - val_accuracy: 0.8432 - val_auc_22: 0.9336\n",
            "Epoch 6/10\n",
            "1370/1370 [==============================] - 6s 4ms/step - loss: 0.3457 - accuracy: 0.8514 - auc_22: 0.9252 - val_loss: 0.3225 - val_accuracy: 0.8615 - val_auc_22: 0.9361\n",
            "Epoch 7/10\n",
            "1370/1370 [==============================] - 6s 4ms/step - loss: 0.3421 - accuracy: 0.8555 - auc_22: 0.9265 - val_loss: 0.3673 - val_accuracy: 0.8446 - val_auc_22: 0.9346\n",
            "Epoch 8/10\n",
            "1370/1370 [==============================] - 6s 4ms/step - loss: 0.3402 - accuracy: 0.8561 - auc_22: 0.9271 - val_loss: 0.3256 - val_accuracy: 0.8557 - val_auc_22: 0.9348\n",
            "Epoch 9/10\n",
            "1370/1370 [==============================] - 6s 4ms/step - loss: 0.3419 - accuracy: 0.8533 - auc_22: 0.9265 - val_loss: 0.3519 - val_accuracy: 0.8555 - val_auc_22: 0.9314\n",
            "Epoch 10/10\n",
            "1370/1370 [==============================] - 6s 4ms/step - loss: 0.3380 - accuracy: 0.8574 - auc_22: 0.9281 - val_loss: 0.3315 - val_accuracy: 0.8670 - val_auc_22: 0.9383\n",
            "Evaluate\n",
            "582/582 [==============================] - 2s 3ms/step - loss: 0.3388 - accuracy: 0.8586 - auc_22: 0.9340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3388303220272064, 0.8586027026176453, 0.9340227842330933]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1on23JWIlp7l",
        "colab_type": "text"
      },
      "source": [
        "# Model F"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeUGcYwri5Bk",
        "colab_type": "code",
        "outputId": "db872fc1-eb9d-45db-ce73-984763e68acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "# construct a model\n",
        "\n",
        "modelE = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='sigmoid', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='sigmoid'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='swish'),\n",
        "    keras.layers.Dense(20, activation='swish'),\n",
        "    keras.layers.Dense(20, activation='swish'),\n",
        "    keras.layers.Dense(20, activation='relu'),\n",
        "    keras.layers.Dense(20, activation='relu'),\n",
        "    keras.layers.Dense(20, activation='relu'),\n",
        "    keras.layers.Dense(20, activation='relu'),\n",
        "    keras.layers.Dense(1, activation ='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "modelE.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', \n",
        "                                                                      tf.keras.metrics.AUC()])\n",
        "\n",
        "modelE.fit(X_train, y_train, epochs=20, batch_size=40, validation_steps = 4, validation_split=0.2, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelE.evaluate(X_test, y_test, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.4843 - accuracy: 0.7584 - auc_23: 0.8459 - val_loss: 0.2736 - val_accuracy: 0.9125 - val_auc_23: 0.9521\n",
            "Epoch 2/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3753 - accuracy: 0.8409 - auc_23: 0.9116 - val_loss: 0.2911 - val_accuracy: 0.8875 - val_auc_23: 0.9529\n",
            "Epoch 3/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3603 - accuracy: 0.8463 - auc_23: 0.9185 - val_loss: 0.2698 - val_accuracy: 0.9062 - val_auc_23: 0.9530\n",
            "Epoch 4/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3526 - accuracy: 0.8493 - auc_23: 0.9222 - val_loss: 0.2858 - val_accuracy: 0.8875 - val_auc_23: 0.9571\n",
            "Epoch 5/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3456 - accuracy: 0.8516 - auc_23: 0.9254 - val_loss: 0.2622 - val_accuracy: 0.9062 - val_auc_23: 0.9604\n",
            "Epoch 6/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3397 - accuracy: 0.8537 - auc_23: 0.9281 - val_loss: 0.2774 - val_accuracy: 0.8938 - val_auc_23: 0.9573\n",
            "Epoch 7/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3346 - accuracy: 0.8578 - auc_23: 0.9301 - val_loss: 0.2705 - val_accuracy: 0.8938 - val_auc_23: 0.9624\n",
            "Epoch 8/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3277 - accuracy: 0.8613 - auc_23: 0.9332 - val_loss: 0.2611 - val_accuracy: 0.8938 - val_auc_23: 0.9613\n",
            "Epoch 9/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3239 - accuracy: 0.8636 - auc_23: 0.9346 - val_loss: 0.2623 - val_accuracy: 0.8938 - val_auc_23: 0.9608\n",
            "Epoch 10/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3167 - accuracy: 0.8680 - auc_23: 0.9372 - val_loss: 0.2661 - val_accuracy: 0.8938 - val_auc_23: 0.9608\n",
            "Epoch 11/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3165 - accuracy: 0.8661 - auc_23: 0.9376 - val_loss: 0.2731 - val_accuracy: 0.8875 - val_auc_23: 0.9614\n",
            "Epoch 12/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3166 - accuracy: 0.8676 - auc_23: 0.9376 - val_loss: 0.2890 - val_accuracy: 0.8938 - val_auc_23: 0.9584\n",
            "Epoch 13/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3149 - accuracy: 0.8670 - auc_23: 0.9382 - val_loss: 0.2511 - val_accuracy: 0.9000 - val_auc_23: 0.9641\n",
            "Epoch 14/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3107 - accuracy: 0.8694 - auc_23: 0.9397 - val_loss: 0.2619 - val_accuracy: 0.8938 - val_auc_23: 0.9622\n",
            "Epoch 15/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3095 - accuracy: 0.8720 - auc_23: 0.9403 - val_loss: 0.2726 - val_accuracy: 0.9000 - val_auc_23: 0.9575\n",
            "Epoch 16/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3071 - accuracy: 0.8697 - auc_23: 0.9411 - val_loss: 0.2792 - val_accuracy: 0.8750 - val_auc_23: 0.9584\n",
            "Epoch 17/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3066 - accuracy: 0.8714 - auc_23: 0.9414 - val_loss: 0.3003 - val_accuracy: 0.8875 - val_auc_23: 0.9573\n",
            "Epoch 18/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3059 - accuracy: 0.8706 - auc_23: 0.9415 - val_loss: 0.2775 - val_accuracy: 0.8938 - val_auc_23: 0.9589\n",
            "Epoch 19/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3016 - accuracy: 0.8731 - auc_23: 0.9434 - val_loss: 0.2697 - val_accuracy: 0.8938 - val_auc_23: 0.9613\n",
            "Epoch 20/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3021 - accuracy: 0.8728 - auc_23: 0.9430 - val_loss: 0.2725 - val_accuracy: 0.8687 - val_auc_23: 0.9560\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 3ms/step - loss: 0.3164 - accuracy: 0.8631 - auc_23: 0.9382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3163735270500183, 0.8630980849266052, 0.9381943345069885]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiWU9OOIKI3h",
        "colab_type": "text"
      },
      "source": [
        "# Model G"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSfRz5uVKE2i",
        "colab_type": "code",
        "outputId": "2c7d0cca-6408-4432-c86c-503a33116d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "# construct a model\n",
        "#deleted 2 layers from initial relu bunch\n",
        "\n",
        "modelG = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='sigmoid', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='sigmoid'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='swish'),\n",
        "    keras.layers.Dense(20, activation='swish'),\n",
        "    keras.layers.Dense(20, activation='swish'),\n",
        "    keras.layers.Dense(20, activation='relu'),\n",
        "    keras.layers.Dense(20, activation='relu'),\n",
        "    keras.layers.Dense(20, activation='relu'),\n",
        "    keras.layers.Dense(20, activation='relu'),\n",
        "    keras.layers.Dense(1, activation ='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "modelG.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', \n",
        "                                                                      tf.keras.metrics.AUC()])\n",
        "\n",
        "modelG.fit(X_train, y_train, epochs=20, batch_size=40, validation_steps = 4, validation_split=0.2, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelG.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.4787 - accuracy: 0.7542 - auc_24: 0.8475 - val_loss: 0.2912 - val_accuracy: 0.8938 - val_auc_24: 0.9528\n",
            "Epoch 2/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3757 - accuracy: 0.8398 - auc_24: 0.9111 - val_loss: 0.2788 - val_accuracy: 0.8875 - val_auc_24: 0.9578\n",
            "Epoch 3/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3630 - accuracy: 0.8464 - auc_24: 0.9172 - val_loss: 0.2773 - val_accuracy: 0.9000 - val_auc_24: 0.9560\n",
            "Epoch 4/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3561 - accuracy: 0.8470 - auc_24: 0.9205 - val_loss: 0.2636 - val_accuracy: 0.8938 - val_auc_24: 0.9575\n",
            "Epoch 5/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3508 - accuracy: 0.8499 - auc_24: 0.9229 - val_loss: 0.2742 - val_accuracy: 0.9000 - val_auc_24: 0.9572\n",
            "Epoch 6/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3453 - accuracy: 0.8521 - auc_24: 0.9254 - val_loss: 0.2711 - val_accuracy: 0.9000 - val_auc_24: 0.9590\n",
            "Epoch 7/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3422 - accuracy: 0.8536 - auc_24: 0.9268 - val_loss: 0.2695 - val_accuracy: 0.8813 - val_auc_24: 0.9575\n",
            "Epoch 8/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3375 - accuracy: 0.8575 - auc_24: 0.9288 - val_loss: 0.2648 - val_accuracy: 0.9000 - val_auc_24: 0.9570\n",
            "Epoch 9/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3366 - accuracy: 0.8552 - auc_24: 0.9293 - val_loss: 0.2696 - val_accuracy: 0.8875 - val_auc_24: 0.9584\n",
            "Epoch 10/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3341 - accuracy: 0.8588 - auc_24: 0.9302 - val_loss: 0.2832 - val_accuracy: 0.8813 - val_auc_24: 0.9563\n",
            "Epoch 11/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3306 - accuracy: 0.8618 - auc_24: 0.9317 - val_loss: 0.2610 - val_accuracy: 0.8938 - val_auc_24: 0.9575\n",
            "Epoch 12/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3291 - accuracy: 0.8598 - auc_24: 0.9324 - val_loss: 0.2781 - val_accuracy: 0.8813 - val_auc_24: 0.9557\n",
            "Epoch 13/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3237 - accuracy: 0.8631 - auc_24: 0.9347 - val_loss: 0.2805 - val_accuracy: 0.8938 - val_auc_24: 0.9534\n",
            "Epoch 14/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3245 - accuracy: 0.8625 - auc_24: 0.9342 - val_loss: 0.2685 - val_accuracy: 0.9000 - val_auc_24: 0.9547\n",
            "Epoch 15/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3213 - accuracy: 0.8650 - auc_24: 0.9359 - val_loss: 0.2674 - val_accuracy: 0.8875 - val_auc_24: 0.9552\n",
            "Epoch 16/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3168 - accuracy: 0.8663 - auc_24: 0.9375 - val_loss: 0.2673 - val_accuracy: 0.8750 - val_auc_24: 0.9563\n",
            "Epoch 17/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3153 - accuracy: 0.8676 - auc_24: 0.9380 - val_loss: 0.2755 - val_accuracy: 0.8687 - val_auc_24: 0.9530\n",
            "Epoch 18/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3101 - accuracy: 0.8700 - auc_24: 0.9402 - val_loss: 0.2705 - val_accuracy: 0.8875 - val_auc_24: 0.9537\n",
            "Epoch 19/20\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.3116 - accuracy: 0.8690 - auc_24: 0.9395 - val_loss: 0.2638 - val_accuracy: 0.8938 - val_auc_24: 0.9576\n",
            "Epoch 20/20\n",
            "594/594 [==============================] - 3s 4ms/step - loss: 0.3076 - accuracy: 0.8717 - auc_24: 0.9410 - val_loss: 0.2787 - val_accuracy: 0.8813 - val_auc_24: 0.9551\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 3ms/step - loss: 0.3409 - accuracy: 0.8471 - auc_24: 0.9361\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3409321904182434, 0.8471490144729614, 0.9360639452934265]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpbw11qMdRB8",
        "colab_type": "text"
      },
      "source": [
        "# model G\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtGjyeGZk27F",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtJjyATGdVS2",
        "colab_type": "code",
        "outputId": "5727e5f2-82b4-49da-9496-43618cf22284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "# construct a model\n",
        "#deleted 2 layers from initial relu bunch\n",
        "\n",
        "modelG = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='sigmoid', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='swish'),\n",
        "    keras.layers.Dense(len(X_df.columns), activation='swish'),\n",
        "    keras.layers.Dense(2 * len(X_df.columns), activation='sigmoid'),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(2*len(X_df.columns), activation='swish'),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(40, activation='swish'),\n",
        "    keras.layers.Dense(20, activation='swish'),\n",
        "    keras.layers.Dense(20, activation='swish'),\n",
        "    keras.layers.Dense(20, activation='swish'),\n",
        "    keras.layers.Dense(20, activation='swish'),\n",
        "    keras.layers.Dense(20, activation='swish'),\n",
        "    keras.layers.Dense(1, activation ='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "modelG.compile(loss='kld', optimizer='adam', metrics=['accuracy', \n",
        "                                                                      tf.keras.metrics.AUC()])\n",
        "\n",
        "modelG.fit(X_train, y_train, epochs=30, batch_size=40, validation_steps = 4, validation_split=0.2, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelG.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "594/594 [==============================] - 3s 5ms/step - loss: 0.0122 - accuracy: 0.4854 - auc_15: 0.4967 - val_loss: -8.8650e-07 - val_accuracy: 0.4500 - val_auc_15: 0.5000\n",
            "Epoch 2/30\n",
            "594/594 [==============================] - 3s 5ms/step - loss: -7.7919e-07 - accuracy: 0.4857 - auc_15: 0.5000 - val_loss: -8.8650e-07 - val_accuracy: 0.4500 - val_auc_15: 0.5000\n",
            "Epoch 3/30\n",
            "594/594 [==============================] - 3s 4ms/step - loss: -7.8766e-07 - accuracy: 0.4857 - auc_15: 0.5000 - val_loss: -8.8650e-07 - val_accuracy: 0.4500 - val_auc_15: 0.5000\n",
            "Epoch 4/30\n",
            "594/594 [==============================] - 3s 5ms/step - loss: -7.9133e-07 - accuracy: 0.4857 - auc_15: 0.5000 - val_loss: -8.8650e-07 - val_accuracy: 0.4500 - val_auc_15: 0.5000\n",
            "Epoch 5/30\n",
            "594/594 [==============================] - 3s 5ms/step - loss: -7.9653e-07 - accuracy: 0.4857 - auc_15: 0.5000 - val_loss: -8.8650e-07 - val_accuracy: 0.4500 - val_auc_15: 0.5000\n",
            "Epoch 6/30\n",
            "594/594 [==============================] - 3s 4ms/step - loss: -7.9668e-07 - accuracy: 0.4857 - auc_15: 0.5000 - val_loss: -8.8650e-07 - val_accuracy: 0.4500 - val_auc_15: 0.5000\n",
            "Epoch 7/30\n",
            "594/594 [==============================] - 3s 4ms/step - loss: -8.0554e-07 - accuracy: 0.4857 - auc_15: 0.5000 - val_loss: -8.8650e-07 - val_accuracy: 0.4500 - val_auc_15: 0.5000\n",
            "Epoch 8/30\n",
            "594/594 [==============================] - 3s 4ms/step - loss: -8.0839e-07 - accuracy: 0.4857 - auc_15: 0.5000 - val_loss: -8.8650e-07 - val_accuracy: 0.4500 - val_auc_15: 0.5000\n",
            "Epoch 9/30\n",
            "526/594 [=========================>....] - ETA: 0s - loss: -8.1370e-07 - accuracy: 0.4855 - auc_15: 0.5000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-bb694a0fac81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                                       tf.keras.metrics.AUC()])\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodelG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-COXoaYGk4ad",
        "colab_type": "code",
        "outputId": "2b62d275-db65-44d2-a685-ab06a36f5f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "# construct a model\n",
        "modelK = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "modelK.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modelK.fit(X_train, y_train, epochs=25, batch_size=100, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelK.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "297/297 [==============================] - 4s 15ms/step - loss: 0.4330 - accuracy: 0.8039\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.3488 - accuracy: 0.8521\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.3293 - accuracy: 0.8611\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.3138 - accuracy: 0.8675\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.3029 - accuracy: 0.8717\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 4s 15ms/step - loss: 0.2877 - accuracy: 0.8792\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.2765 - accuracy: 0.8832\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.2626 - accuracy: 0.8872\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.2497 - accuracy: 0.8942\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.2318 - accuracy: 0.9010\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.2140 - accuracy: 0.9096\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.1932 - accuracy: 0.9210\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.1751 - accuracy: 0.9295\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.1555 - accuracy: 0.9383\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.1302 - accuracy: 0.9493\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.1174 - accuracy: 0.9527\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.1086 - accuracy: 0.9593\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.0931 - accuracy: 0.9649\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.0875 - accuracy: 0.9678\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 4s 14ms/step - loss: 0.0789 - accuracy: 0.9708\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 2ms/step - loss: 0.5963 - accuracy: 0.8662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5963054895401001, 0.8661783933639526]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGhHD2Lsn60a",
        "colab_type": "text"
      },
      "source": [
        "# Model K"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFF6c6_6mEt6",
        "colab_type": "code",
        "outputId": "16801779-5f16-4344-fac5-ea0af72704d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        }
      },
      "source": [
        "# construct a model\n",
        "modelK = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "modelK.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modelK.fit(X_train, y_train, epochs=25, batch_size=242, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelK.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "\n",
        "#It can be seen in the epoch's below that as the loss function yeilds\n",
        "#lower and lower results that the model is obviously being overfit. \n",
        "\n",
        "#the significant drop in accuracy on the testing set is also indicative of \n",
        "#overfitting"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.4872 - accuracy: 0.7752\n",
            "Epoch 2/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.3623 - accuracy: 0.8454\n",
            "Epoch 3/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.3367 - accuracy: 0.8583\n",
            "Epoch 4/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.3144 - accuracy: 0.8672\n",
            "Epoch 5/25\n",
            "123/123 [==============================] - 5s 38ms/step - loss: 0.3035 - accuracy: 0.8725\n",
            "Epoch 6/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.2877 - accuracy: 0.8795\n",
            "Epoch 7/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.2746 - accuracy: 0.8843\n",
            "Epoch 8/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.2592 - accuracy: 0.8912\n",
            "Epoch 9/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.2426 - accuracy: 0.8972\n",
            "Epoch 10/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.2290 - accuracy: 0.9042\n",
            "Epoch 11/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.2097 - accuracy: 0.9140\n",
            "Epoch 12/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1903 - accuracy: 0.9207\n",
            "Epoch 13/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1724 - accuracy: 0.9291\n",
            "Epoch 14/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1598 - accuracy: 0.9347\n",
            "Epoch 15/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1373 - accuracy: 0.9451\n",
            "Epoch 16/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1210 - accuracy: 0.9509\n",
            "Epoch 17/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1173 - accuracy: 0.9569\n",
            "Epoch 18/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1014 - accuracy: 0.9607\n",
            "Epoch 19/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0878 - accuracy: 0.9667\n",
            "Epoch 20/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0770 - accuracy: 0.9710\n",
            "Epoch 21/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0729 - accuracy: 0.9720\n",
            "Epoch 22/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0647 - accuracy: 0.9763\n",
            "Epoch 23/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0598 - accuracy: 0.9776\n",
            "Epoch 24/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0523 - accuracy: 0.9804\n",
            "Epoch 25/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0533 - accuracy: 0.9800\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 3ms/step - loss: 0.7719 - accuracy: 0.8754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7718998193740845, 0.8754192590713501]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utiyJQskn-zr",
        "colab_type": "text"
      },
      "source": [
        "To correct for overfitting, adding a dropout layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1FyUiyNn9SS",
        "colab_type": "code",
        "outputId": "7cb2689e-d490-4ad3-bcdc-5859bc2f1833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        }
      },
      "source": [
        "# construct a model\n",
        "modelK = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dropout(0.5,\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "modelK.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modelK.fit(X_train, y_train, epochs=25, batch_size=242, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelK.evaluate(X_test, y_test, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.4942 - accuracy: 0.7669\n",
            "Epoch 2/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.3627 - accuracy: 0.8454\n",
            "Epoch 3/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.3359 - accuracy: 0.8592\n",
            "Epoch 4/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.3140 - accuracy: 0.8677\n",
            "Epoch 5/25\n",
            "123/123 [==============================] - 5s 38ms/step - loss: 0.3057 - accuracy: 0.8726\n",
            "Epoch 6/25\n",
            "123/123 [==============================] - 5s 38ms/step - loss: 0.2891 - accuracy: 0.8801\n",
            "Epoch 7/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.2755 - accuracy: 0.8845\n",
            "Epoch 8/25\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.2607 - accuracy: 0.8916\n",
            "Epoch 9/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.2454 - accuracy: 0.8975\n",
            "Epoch 10/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.2240 - accuracy: 0.9078\n",
            "Epoch 11/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.2100 - accuracy: 0.9142\n",
            "Epoch 12/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1870 - accuracy: 0.9221\n",
            "Epoch 13/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1736 - accuracy: 0.9303\n",
            "Epoch 14/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1520 - accuracy: 0.9385\n",
            "Epoch 15/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1374 - accuracy: 0.9454\n",
            "Epoch 16/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1174 - accuracy: 0.9524\n",
            "Epoch 17/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1133 - accuracy: 0.9571\n",
            "Epoch 18/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0986 - accuracy: 0.9619\n",
            "Epoch 19/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0939 - accuracy: 0.9666\n",
            "Epoch 20/25\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0768 - accuracy: 0.9714\n",
            "Epoch 21/25\n",
            "123/123 [==============================] - 4s 37ms/step - loss: 0.0692 - accuracy: 0.9753\n",
            "Epoch 22/25\n",
            "123/123 [==============================] - 4s 37ms/step - loss: 0.0631 - accuracy: 0.9763\n",
            "Epoch 23/25\n",
            "123/123 [==============================] - 4s 37ms/step - loss: 0.0566 - accuracy: 0.9795\n",
            "Epoch 24/25\n",
            "123/123 [==============================] - 4s 37ms/step - loss: 0.0529 - accuracy: 0.9818\n",
            "Epoch 25/25\n",
            "123/123 [==============================] - 4s 37ms/step - loss: 0.0509 - accuracy: 0.9812\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 3ms/step - loss: 0.7322 - accuracy: 0.8752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7321678996086121, 0.8752139210700989]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJb2QFZ7op8i",
        "colab_type": "code",
        "outputId": "af7d3ff6-cb25-43f8-dc6a-fc4607971f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "#Reducing number of Epochs\n",
        "\n",
        "# construct a model\n",
        "modelK = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "modelK.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "modelK.fit(X_train, y_train, epochs=18, batch_size=242, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelK.evaluate(X_test, y_test, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/18\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.4962 - accuracy: 0.7671\n",
            "Epoch 2/18\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.3679 - accuracy: 0.8452\n",
            "Epoch 3/18\n",
            "123/123 [==============================] - 4s 37ms/step - loss: 0.3406 - accuracy: 0.8566\n",
            "Epoch 4/18\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.3245 - accuracy: 0.8649\n",
            "Epoch 5/18\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.3070 - accuracy: 0.8737\n",
            "Epoch 6/18\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.2960 - accuracy: 0.8771\n",
            "Epoch 7/18\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.2824 - accuracy: 0.8824\n",
            "Epoch 8/18\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.2658 - accuracy: 0.8893\n",
            "Epoch 9/18\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.2497 - accuracy: 0.8971\n",
            "Epoch 10/18\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.2408 - accuracy: 0.9014\n",
            "Epoch 11/18\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.2133 - accuracy: 0.9118\n",
            "Epoch 12/18\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1979 - accuracy: 0.9200\n",
            "Epoch 13/18\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1807 - accuracy: 0.9269\n",
            "Epoch 14/18\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1612 - accuracy: 0.9349\n",
            "Epoch 15/18\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1463 - accuracy: 0.9414\n",
            "Epoch 16/18\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1263 - accuracy: 0.9502\n",
            "Epoch 17/18\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1161 - accuracy: 0.9557\n",
            "Epoch 18/18\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1008 - accuracy: 0.9608\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 3ms/step - loss: 0.5329 - accuracy: 0.8645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5329168438911438, 0.8644670844078064]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c4EQ-NpqKcW",
        "colab_type": "code",
        "outputId": "7a6e70ad-d28d-4e57-fc91-20649e70605a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "#Reducing number of Epochs\n",
        "\n",
        "# construct a model\n",
        "modelK = keras.models.Sequential([\n",
        "    keras.layers.Dense(len(X_df.columns), activation='relu', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(100 * len(X_df.columns), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "modelK.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', ])\n",
        "\n",
        "modelK.fit(X_train, y_train, epochs=20, batch_size=100, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelK.evaluate(X_test, y_test, verbose=1)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.4573 - accuracy: 0.7917\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.3583 - accuracy: 0.8513\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.3337 - accuracy: 0.8611\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.3141 - accuracy: 0.8666\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.3048 - accuracy: 0.8715\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.2919 - accuracy: 0.8782\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.2786 - accuracy: 0.8826\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.2625 - accuracy: 0.8873\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.2487 - accuracy: 0.8945\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.2319 - accuracy: 0.9029\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.2159 - accuracy: 0.9088\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.2028 - accuracy: 0.9170\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.1772 - accuracy: 0.9275\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.1622 - accuracy: 0.9343\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.1421 - accuracy: 0.9450\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.1295 - accuracy: 0.9492\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.1147 - accuracy: 0.9554\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.1016 - accuracy: 0.9613\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.0900 - accuracy: 0.9665\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 5s 18ms/step - loss: 0.0771 - accuracy: 0.9707\n",
            "Evaluate\n",
            "457/457 [==============================] - 1s 3ms/step - loss: 0.5521 - accuracy: 0.8717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.55209881067276, 0.871722936630249]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHl84Hy8rTfa",
        "colab_type": "code",
        "outputId": "d5c9c208-91ef-40b1-83a3-e963490b0a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Reducing number of Epochs\n",
        "\n",
        "# construct a model\n",
        "modelK = keras.models.Sequential([\n",
        "    keras.layers.Dense(2*len(X_df.columns), activation='swish', input_shape=(len(X_df.columns),)),\n",
        "    keras.layers.Dense(100, activation = 'swish'),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(100,activation='swish'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(40, activation = 'swish'),\n",
        "    keras.layers.Dense(len(X_df.columns), activation = 'sigmoid'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "modelK.compile(loss='BinaryCrossentropy', optimizer='nadam', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "\n",
        "modelK.fit(X_train, y_train, epochs=50, batch_size=242, validation_steps=4, validation_split=0.12, \n",
        "           validation_batch_size=100, verbose=1)\n",
        "\n",
        "print('Evaluate')\n",
        "modelK.evaluate(X_test, y_test, verbose=1)\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "108/108 [==============================] - 1s 12ms/step - loss: 0.4654 - accuracy: 0.7680 - auc_22: 0.8565 - val_loss: 0.3785 - val_accuracy: 0.8325 - val_auc_22: 0.9091\n",
            "Epoch 2/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.3633 - accuracy: 0.8470 - auc_22: 0.9167 - val_loss: 0.3479 - val_accuracy: 0.8400 - val_auc_22: 0.9246\n",
            "Epoch 3/50\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.3490 - accuracy: 0.8510 - auc_22: 0.9238 - val_loss: 0.3451 - val_accuracy: 0.8450 - val_auc_22: 0.9271\n",
            "Epoch 4/50\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.3437 - accuracy: 0.8554 - auc_22: 0.9260 - val_loss: 0.3290 - val_accuracy: 0.8575 - val_auc_22: 0.9319\n",
            "Epoch 5/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.3378 - accuracy: 0.8585 - auc_22: 0.9286 - val_loss: 0.3289 - val_accuracy: 0.8550 - val_auc_22: 0.9330\n",
            "Epoch 6/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.3338 - accuracy: 0.8593 - auc_22: 0.9304 - val_loss: 0.3358 - val_accuracy: 0.8475 - val_auc_22: 0.9310\n",
            "Epoch 7/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.3309 - accuracy: 0.8606 - auc_22: 0.9317 - val_loss: 0.3265 - val_accuracy: 0.8525 - val_auc_22: 0.9335\n",
            "Epoch 8/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.3278 - accuracy: 0.8618 - auc_22: 0.9329 - val_loss: 0.3344 - val_accuracy: 0.8450 - val_auc_22: 0.9321\n",
            "Epoch 9/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.3244 - accuracy: 0.8624 - auc_22: 0.9343 - val_loss: 0.3232 - val_accuracy: 0.8550 - val_auc_22: 0.9346\n",
            "Epoch 10/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.3216 - accuracy: 0.8656 - auc_22: 0.9354 - val_loss: 0.3226 - val_accuracy: 0.8600 - val_auc_22: 0.9368\n",
            "Epoch 11/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.3218 - accuracy: 0.8643 - auc_22: 0.9355 - val_loss: 0.3186 - val_accuracy: 0.8575 - val_auc_22: 0.9372\n",
            "Epoch 12/50\n",
            "108/108 [==============================] - 1s 12ms/step - loss: 0.3188 - accuracy: 0.8678 - auc_22: 0.9367 - val_loss: 0.3133 - val_accuracy: 0.8525 - val_auc_22: 0.9386\n",
            "Epoch 13/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.3136 - accuracy: 0.8680 - auc_22: 0.9387 - val_loss: 0.3163 - val_accuracy: 0.8600 - val_auc_22: 0.9375\n",
            "Epoch 14/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.3130 - accuracy: 0.8677 - auc_22: 0.9390 - val_loss: 0.3126 - val_accuracy: 0.8575 - val_auc_22: 0.9381\n",
            "Epoch 15/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.3120 - accuracy: 0.8687 - auc_22: 0.9395 - val_loss: 0.3139 - val_accuracy: 0.8525 - val_auc_22: 0.9374\n",
            "Epoch 16/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.3097 - accuracy: 0.8710 - auc_22: 0.9403 - val_loss: 0.3132 - val_accuracy: 0.8600 - val_auc_22: 0.9376\n",
            "Epoch 17/50\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.3080 - accuracy: 0.8701 - auc_22: 0.9411 - val_loss: 0.3094 - val_accuracy: 0.8625 - val_auc_22: 0.9402\n",
            "Epoch 18/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.3057 - accuracy: 0.8719 - auc_22: 0.9419 - val_loss: 0.3228 - val_accuracy: 0.8650 - val_auc_22: 0.9362\n",
            "Epoch 19/50\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.3030 - accuracy: 0.8738 - auc_22: 0.9428 - val_loss: 0.3185 - val_accuracy: 0.8525 - val_auc_22: 0.9366\n",
            "Epoch 20/50\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.3008 - accuracy: 0.8754 - auc_22: 0.9439 - val_loss: 0.3088 - val_accuracy: 0.8800 - val_auc_22: 0.9417\n",
            "Epoch 21/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2997 - accuracy: 0.8757 - auc_22: 0.9443 - val_loss: 0.3006 - val_accuracy: 0.8825 - val_auc_22: 0.9450\n",
            "Epoch 22/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2992 - accuracy: 0.8756 - auc_22: 0.9445 - val_loss: 0.3093 - val_accuracy: 0.8600 - val_auc_22: 0.9398\n",
            "Epoch 23/50\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.2966 - accuracy: 0.8777 - auc_22: 0.9454 - val_loss: 0.3077 - val_accuracy: 0.8675 - val_auc_22: 0.9436\n",
            "Epoch 24/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2951 - accuracy: 0.8772 - auc_22: 0.9461 - val_loss: 0.3175 - val_accuracy: 0.8675 - val_auc_22: 0.9436\n",
            "Epoch 25/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2920 - accuracy: 0.8792 - auc_22: 0.9469 - val_loss: 0.3052 - val_accuracy: 0.8650 - val_auc_22: 0.9415\n",
            "Epoch 26/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2928 - accuracy: 0.8796 - auc_22: 0.9467 - val_loss: 0.3040 - val_accuracy: 0.8750 - val_auc_22: 0.9401\n",
            "Epoch 27/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2914 - accuracy: 0.8796 - auc_22: 0.9472 - val_loss: 0.2993 - val_accuracy: 0.8650 - val_auc_22: 0.9438\n",
            "Epoch 28/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2896 - accuracy: 0.8812 - auc_22: 0.9478 - val_loss: 0.2911 - val_accuracy: 0.8825 - val_auc_22: 0.9490\n",
            "Epoch 29/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2884 - accuracy: 0.8807 - auc_22: 0.9484 - val_loss: 0.3044 - val_accuracy: 0.8650 - val_auc_22: 0.9422\n",
            "Epoch 30/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2867 - accuracy: 0.8828 - auc_22: 0.9489 - val_loss: 0.2928 - val_accuracy: 0.8775 - val_auc_22: 0.9465\n",
            "Epoch 31/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2857 - accuracy: 0.8821 - auc_22: 0.9493 - val_loss: 0.3086 - val_accuracy: 0.8725 - val_auc_22: 0.9418\n",
            "Epoch 32/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2848 - accuracy: 0.8819 - auc_22: 0.9496 - val_loss: 0.2936 - val_accuracy: 0.8675 - val_auc_22: 0.9452\n",
            "Epoch 33/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2847 - accuracy: 0.8825 - auc_22: 0.9497 - val_loss: 0.2880 - val_accuracy: 0.8800 - val_auc_22: 0.9484\n",
            "Epoch 34/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2840 - accuracy: 0.8841 - auc_22: 0.9497 - val_loss: 0.3011 - val_accuracy: 0.8700 - val_auc_22: 0.9457\n",
            "Epoch 35/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.2820 - accuracy: 0.8846 - auc_22: 0.9506 - val_loss: 0.2955 - val_accuracy: 0.8700 - val_auc_22: 0.9477\n",
            "Epoch 36/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2817 - accuracy: 0.8838 - auc_22: 0.9506 - val_loss: 0.2930 - val_accuracy: 0.8850 - val_auc_22: 0.9478\n",
            "Epoch 37/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.2805 - accuracy: 0.8859 - auc_22: 0.9509 - val_loss: 0.2992 - val_accuracy: 0.8725 - val_auc_22: 0.9469\n",
            "Epoch 38/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.2796 - accuracy: 0.8866 - auc_22: 0.9513 - val_loss: 0.2982 - val_accuracy: 0.8800 - val_auc_22: 0.9469\n",
            "Epoch 39/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2773 - accuracy: 0.8856 - auc_22: 0.9521 - val_loss: 0.2996 - val_accuracy: 0.8675 - val_auc_22: 0.9434\n",
            "Epoch 40/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2785 - accuracy: 0.8863 - auc_22: 0.9518 - val_loss: 0.2818 - val_accuracy: 0.8800 - val_auc_22: 0.9501\n",
            "Epoch 41/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.2772 - accuracy: 0.8870 - auc_22: 0.9523 - val_loss: 0.2902 - val_accuracy: 0.8750 - val_auc_22: 0.9483\n",
            "Epoch 42/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.2777 - accuracy: 0.8855 - auc_22: 0.9519 - val_loss: 0.2937 - val_accuracy: 0.8775 - val_auc_22: 0.9463\n",
            "Epoch 43/50\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 0.2754 - accuracy: 0.8875 - auc_22: 0.9528 - val_loss: 0.3058 - val_accuracy: 0.8800 - val_auc_22: 0.9426\n",
            "Epoch 44/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.2737 - accuracy: 0.8890 - auc_22: 0.9533 - val_loss: 0.2910 - val_accuracy: 0.8750 - val_auc_22: 0.9474\n",
            "Epoch 45/50\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.2732 - accuracy: 0.8877 - auc_22: 0.9535 - val_loss: 0.3023 - val_accuracy: 0.8650 - val_auc_22: 0.9428\n",
            "Epoch 46/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.2752 - accuracy: 0.8864 - auc_22: 0.9530 - val_loss: 0.3007 - val_accuracy: 0.8725 - val_auc_22: 0.9446\n",
            "Epoch 47/50\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.2734 - accuracy: 0.8883 - auc_22: 0.9536 - val_loss: 0.2973 - val_accuracy: 0.8800 - val_auc_22: 0.9455\n",
            "Epoch 48/50\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.2716 - accuracy: 0.8885 - auc_22: 0.9542 - val_loss: 0.2916 - val_accuracy: 0.8725 - val_auc_22: 0.9481\n",
            "Epoch 49/50\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.2720 - accuracy: 0.8887 - auc_22: 0.9538 - val_loss: 0.3089 - val_accuracy: 0.8750 - val_auc_22: 0.9422\n",
            "Epoch 50/50\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.2685 - accuracy: 0.8903 - auc_22: 0.9552 - val_loss: 0.3073 - val_accuracy: 0.8650 - val_auc_22: 0.9403\n",
            "Evaluate\n",
            "457/457 [==============================] - 2s 4ms/step - loss: 0.3130 - accuracy: 0.8717 - auc_22: 0.9405\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31299808621406555, 0.871722936630249, 0.9404857754707336]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    }
  ]
}